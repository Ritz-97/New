{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPybq9rYHS/w4Tcaz+ZcRDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritz-97/New/blob/master/ThoughtVizImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzeFEiPExi4h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from random import randint\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import utils.data_input_util as inutil\n",
        "from training.models.thoughtviz import *\n",
        "from utils.image_utils import *\n",
        "\n",
        "\n",
        "def train_gan(dataset, input_noise_dim, batch_size, epochs, model_save_dir, output_dir, classifier_model_file):\n",
        "\n",
        "    # folders containing images used for training\n",
        "    char_fonts_folders = [\"./images/Char-Font\"]\n",
        "    num_classes = 10\n",
        "\n",
        "    # load data and compile discriminator, generator models depending on the dataaset\n",
        "    if dataset == 0:\n",
        "        x_train, y_train, x_test, y_test = inutil.load_digit_data()\n",
        "        print(\"Loaded Digits Dataset.\", )\n",
        "\n",
        "    if dataset == 1:\n",
        "        x_train, y_train, x_test, y_test = inutil.load_char_data(char_fonts_folders, resize_shape=(28, 28))\n",
        "        print(\"Loaded Characters Dataset.\", )\n",
        "\n",
        "    adam_lr = 0.0002\n",
        "    adam_beta_1 = 0.5\n",
        "    \n",
        "    c = load_model(classifier_model_file)\n",
        "\n",
        "    d = discriminator_model((28, 28), c)\n",
        "    d_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
        "    d.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=d_optim)\n",
        "    d.trainable = True\n",
        "\n",
        "    g = generator_model(input_noise_dim, num_classes)\n",
        "    g_optim = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
        "    g.compile(loss='categorical_crossentropy', optimizer=g_optim)\n",
        "\n",
        "    d_on_g = generator_containing_discriminator(input_noise_dim, num_classes, g, d)\n",
        "    d_on_g.compile(loss=['binary_crossentropy','categorical_crossentropy'], optimizer=g_optim)\n",
        "\n",
        "    g.summary()\n",
        "    d.summary()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch is \", epoch)\n",
        "\n",
        "        print(\"Number of batches\", int(x_train.shape[0]/batch_size))\n",
        "\n",
        "        for index in range(int(x_train.shape[0]/batch_size)):\n",
        "            # generate noise from a normal distribution\n",
        "            noise = np.random.uniform(-1, 1, (batch_size, input_noise_dim))\n",
        "\n",
        "            random_labels = [randint(0, 9) for i in range(batch_size)]\n",
        "\n",
        "            one_hot_vectors = [to_categorical(label, 10) for label in random_labels]\n",
        "\n",
        "            # get real images and corresponding labels\n",
        "            real_images = x_train[index * batch_size:(index + 1) * batch_size]\n",
        "            real_labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            # generate fake images using the generator\n",
        "            generated_images = g.predict(, verbose=0)\n",
        "\n",
        "            # discriminator loss of real images\n",
        "            d_loss_real = d.train_on_batch(real_images, [np.array([1] * batch_size), np.array(real_labels)])\n",
        "            # discriminator loss of fake images\n",
        "            d_loss_fake = d.train_on_batch(generated_images, [np.array([0] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
        "            d_loss = (d_loss_fake[0] + d_loss_real[0]) * 0.5\n",
        "\n",
        "            # save generated images at intermediate stages of training\n",
        "            if index % 250 == 0:\n",
        "                image = combine_images(generated_images)\n",
        "                image = image * 127.5 + 127.5\n",
        "                img_save_path = os.path.join(output_dir, str(epoch) + \"_g_\" + str(index) + \".png\")\n",
        "                Image.fromarray(image.astype(np.uint8)).save(img_save_path)\n",
        "\n",
        "            d.trainable = False\n",
        "            # generator loss\n",
        "            g_loss = d_on_g.train_on_batch([np.array(noise), np.array(one_hot_vectors)], [np.array([1] * batch_size), np.array(one_hot_vectors).reshape(batch_size, num_classes)])\n",
        "            d.trainable = True\n",
        "\n",
        "        print(\"Epoch %d d_loss : %f\" % (epoch, d_loss))\n",
        "        print(\"Epoch %d g_loss : %f\" % (epoch, g_loss[0]))\n",
        "\n",
        "        # save generator and discriminator models along with the weights\n",
        "        g.save(os.path.join(model_save_dir, 'generator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
        "        d.save(os.path.join(model_save_dir, 'discriminator_' + str(epoch)), overwrite=True, include_optimizer=True)\n",
        "\n",
        "\n",
        "def train():\n",
        "    folder_name_mapping = {0: 'Digit', 1: 'Char'}\n",
        "    dataset = 1\n",
        "    batch_size = 100\n",
        "    run_id = 1\n",
        "    epochs = 500\n",
        "    model_save_dir = os.path.join('./saved_models/thoughtviz_with_label/', folder_name_mapping[dataset], 'run_' + str(run_id))\n",
        "    if not os.path.exists(model_save_dir):\n",
        "        os.makedirs(model_save_dir)\n",
        "\n",
        "    output_dir = os.path.join('./outputs/thoughtviz_with_label/', folder_name_mapping[dataset], 'run_' + str(run_id))\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    classifier_model_file = os.path.join('./trained_classifier_models', 'classifier_' + folder_name_mapping[dataset].lower() + '.h5')\n",
        "\n",
        "    train_gan(dataset=dataset, input_noise_dim=100, batch_size=batch_size, epochs=epochs, model_save_dir=model_save_dir, output_dir=output_dir, classifier_model_file=classifier_model_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    }
  ]
}